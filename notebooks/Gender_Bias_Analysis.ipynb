{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34276594",
   "metadata": {},
   "source": [
    "# Gender Bias in Large Language Models - Comprehensive Analysis\n",
    "\n",
    "This notebook provides a complete analysis framework for the Gender Bias in LLMs study. It includes data loading, statistical analysis, visualization, and interpretation of results.\n",
    "\n",
    "## Study Overview\n",
    "\n",
    "This research investigates how different prompting strategies affect gender bias in LLM outputs:\n",
    "\n",
    "1. **Raw Prompt** (Control) - Basic rewrite request\n",
    "2. **System Prompt** - Explicit gender-neutral instructions  \n",
    "3. **Few-Shot** - Examples + instructions\n",
    "4. **Few-Shot + Verification** - Examples + self-verification\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "- **Gender Bias Score** - Automated detection of gendered terms\n",
    "- **Fluency Score** - Text quality assessment\n",
    "- **BLEU-4 Score** - Meaning preservation\n",
    "- **Semantic Similarity** - Content preservation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76fa00f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Setting up the analysis environment with all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import f_oneway, ttest_ind\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c287f1c",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "Loading experiment results and exploring the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b445e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent results file\n",
    "results_dir = project_root / \"data\" / \"results\"\n",
    "results_files = list(results_dir.glob(\"experiment_results_*.json\"))\n",
    "\n",
    "if not results_files:\n",
    "    print(\"‚ùå No experiment results found!\")\n",
    "    print(f\"Please run the experiment first: python {project_root}/main.py run-experiment\")\n",
    "else:\n",
    "    # Load most recent results\n",
    "    latest_results_file = max(results_files, key=lambda f: f.stat().st_mtime)\n",
    "    print(f\"üìÅ Loading results from: {latest_results_file.name}\")\n",
    "    \n",
    "    with open(latest_results_file, 'r') as f:\n",
    "        results_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úì Loaded experiment data:\")\n",
    "    print(f\"  - Experiment ID: {results_data['experiment_id']}\")\n",
    "    print(f\"  - Status: {results_data['status']}\")\n",
    "    print(f\"  - Total experiments: {results_data['total_experiments']}\")\n",
    "    print(f\"  - Timestamp: {results_data['timestamp']}\")\n",
    "    \n",
    "    # Show configuration\n",
    "    config = results_data['configuration']\n",
    "    print(f\"\\nüìã Experiment Configuration:\")\n",
    "    print(f\"  - Repetitions per paragraph: {config['repetitions_per_paragraph']}\")\n",
    "    print(f\"  - Strategies: {', '.join(config['prompt_strategies'])}\")\n",
    "    print(f\"  - Models: {', '.join(config['llm_models'])}\")\n",
    "    print(f\"  - Temperature: {config['temperature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8356ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "def create_dataframe_from_results(results_data):\n",
    "    \"\"\"Convert experiment results to pandas DataFrame\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for result in results_data.get(\"detailed_results\", []):\n",
    "        if result.get(\"success\", False):\n",
    "            evaluation = result[\"evaluation\"]\n",
    "            summary_scores = evaluation[\"summary_scores\"]\n",
    "            \n",
    "            row = {\n",
    "                \"experiment_id\": result[\"experiment_id\"],\n",
    "                \"paragraph_id\": result[\"paragraph_id\"],\n",
    "                \"strategy\": result[\"strategy\"],\n",
    "                \"model\": result[\"model\"],\n",
    "                \"repetition\": result[\"repetition\"],\n",
    "                \"bias_reduction_percentage\": summary_scores[\"bias_reduction_percentage\"],\n",
    "                \"is_gender_neutral\": summary_scores[\"is_gender_neutral\"],\n",
    "                \"fluency_score\": summary_scores[\"fluency_score\"],\n",
    "                \"bleu_4_score\": summary_scores[\"bleu_4_score\"],\n",
    "                \"semantic_similarity\": summary_scores[\"semantic_similarity\"],\n",
    "                \"generation_time\": result[\"generation_time\"],\n",
    "                \n",
    "                # Additional metrics\n",
    "                \"original_bias_score\": evaluation[\"bias_evaluation\"][\"original_bias\"][\"bias_score\"],\n",
    "                \"generated_bias_score\": evaluation[\"bias_evaluation\"][\"generated_bias\"][\"bias_score\"],\n",
    "                \"total_gendered_terms_original\": evaluation[\"bias_evaluation\"][\"original_bias\"][\"total_gendered_terms\"],\n",
    "                \"total_gendered_terms_generated\": evaluation[\"bias_evaluation\"][\"generated_bias\"][\"total_gendered_terms\"],\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create DataFrame\n",
    "if 'results_data' in locals():\n",
    "    df = create_dataframe_from_results(results_data)\n",
    "    \n",
    "    print(f\"üìä DataFrame created with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(f\"\\nüîç Data Overview:\")\n",
    "    print(f\"  - Strategies: {df['strategy'].unique()}\")\n",
    "    print(f\"  - Models: {df['model'].unique()}\")\n",
    "    print(f\"  - Paragraphs: {df['paragraph_id'].nunique()}\")\n",
    "    print(f\"  - Total repetitions: {df['repetition'].max()}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\\nüìã Sample Data:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìà Summary Statistics:\")\n",
    "    display(df[['bias_reduction_percentage', 'fluency_score', 'bleu_4_score', 'semantic_similarity']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191068f",
   "metadata": {},
   "source": [
    "## 3. Statistical Analysis\n",
    "\n",
    "Performing comprehensive statistical analysis including ANOVA tests and post-hoc comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf35f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_anova_analysis(df, dependent_var, independent_var=\"strategy\"):\n",
    "    \"\"\"Perform ANOVA test and return results\"\"\"\n",
    "    \n",
    "    # Group data by independent variable\n",
    "    groups = []\n",
    "    group_names = []\n",
    "    \n",
    "    for group_name in df[independent_var].unique():\n",
    "        group_data = df[df[independent_var] == group_name][dependent_var]\n",
    "        groups.append(group_data)\n",
    "        group_names.append(group_name)\n",
    "    \n",
    "    # Perform one-way ANOVA\n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "    \n",
    "    # Calculate effect size (eta-squared)\n",
    "    ss_between = sum(len(group) * (group.mean() - df[dependent_var].mean())**2 for group in groups)\n",
    "    ss_total = ((df[dependent_var] - df[dependent_var].mean())**2).sum()\n",
    "    eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "    \n",
    "    # Group statistics\n",
    "    group_stats = {}\n",
    "    for i, group_name in enumerate(group_names):\n",
    "        group_stats[group_name] = {\n",
    "            \"mean\": groups[i].mean(),\n",
    "            \"std\": groups[i].std(),\n",
    "            \"count\": len(groups[i])\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"dependent_variable\": dependent_var,\n",
    "        \"f_statistic\": f_stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"eta_squared\": eta_squared,\n",
    "        \"significant\": p_value < 0.05,\n",
    "        \"group_statistics\": group_stats,\n",
    "        \"groups\": groups,\n",
    "        \"group_names\": group_names\n",
    "    }\n",
    "\n",
    "def perform_post_hoc_tests(df, dependent_var, independent_var=\"strategy\"):\n",
    "    \"\"\"Perform post-hoc pairwise comparisons\"\"\"\n",
    "    \n",
    "    # Use Tukey's HSD test\n",
    "    tukey_result = pairwise_tukeyhsd(\n",
    "        endog=df[dependent_var],\n",
    "        groups=df[independent_var],\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    return tukey_result\n",
    "\n",
    "# Perform ANOVA for each metric\n",
    "if 'df' in locals():\n",
    "    metrics = [\"bias_reduction_percentage\", \"fluency_score\", \"bleu_4_score\", \"semantic_similarity\"]\n",
    "    anova_results = {}\n",
    "    \n",
    "    print(\"üî¨ Performing ANOVA Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\nüìä {metric.replace('_', ' ').title()}:\")\n",
    "        \n",
    "        anova_result = perform_anova_analysis(df, metric)\n",
    "        anova_results[metric] = anova_result\n",
    "        \n",
    "        print(f\"  F-statistic: {anova_result['f_statistic']:.4f}\")\n",
    "        print(f\"  p-value: {anova_result['p_value']:.6f}\")\n",
    "        print(f\"  Effect size (Œ∑¬≤): {anova_result['eta_squared']:.4f}\")\n",
    "        \n",
    "        if anova_result['significant']:\n",
    "            print(f\"  ‚úÖ SIGNIFICANT difference between strategies (p < 0.05)\")\n",
    "            \n",
    "            # Perform post-hoc tests\n",
    "            tukey_result = perform_post_hoc_tests(df, metric)\n",
    "            print(f\"  üìã Post-hoc comparisons (Tukey's HSD):\")\n",
    "            print(f\"     {tukey_result}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå No significant difference between strategies\")\n",
    "        \n",
    "        # Show group means\n",
    "        print(f\"  üìà Group means:\")\n",
    "        for group, stats in anova_result['group_statistics'].items():\n",
    "            print(f\"     {group}: {stats['mean']:.3f} (¬±{stats['std']:.3f})\")\n",
    "\n",
    "    print(f\"\\n‚úÖ ANOVA analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debfb4a3",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Visualizations\n",
    "\n",
    "Creating multiple visualizations to understand the results from different perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Strategy Comparison - Box Plots\n",
    "if 'df' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    metrics = [\"bias_reduction_percentage\", \"fluency_score\", \"bleu_4_score\", \"semantic_similarity\"]\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        sns.boxplot(data=df, x=\"strategy\", y=metric, ax=axes[i])\n",
    "        axes[i].set_title(f'{metric.replace(\"_\", \" \").title()} by Strategy')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add significance markers if ANOVA was significant\n",
    "        if 'anova_results' in locals() and anova_results[metric]['significant']:\n",
    "            axes[i].text(0.02, 0.98, '***', transform=axes[i].transAxes, \n",
    "                        fontsize=16, fontweight='bold', va='top', color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Strategy comparison box plots generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Interactive Scatter Plot - Trade-off Analysis\n",
    "if 'df' in locals():\n",
    "    fig = px.scatter(\n",
    "        df, \n",
    "        x=\"bias_reduction_percentage\", \n",
    "        y=\"fluency_score\",\n",
    "        color=\"strategy\", \n",
    "        size=\"bleu_4_score\",\n",
    "        hover_data=[\"paragraph_id\", \"model\", \"semantic_similarity\"],\n",
    "        title=\"Trade-off Analysis: Bias Reduction vs Fluency\",\n",
    "        labels={\n",
    "            \"bias_reduction_percentage\": \"Bias Reduction (%)\",\n",
    "            \"fluency_score\": \"Fluency Score\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"üéØ Interactive trade-off analysis generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Performance Dashboard\n",
    "if 'df' in locals():\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Mean Bias Reduction by Strategy', 'Success Rate by Strategy',\n",
    "                       'Quality Metrics by Strategy', 'Performance Distribution'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"histogram\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Mean bias reduction\n",
    "    strategy_bias = df.groupby(\"strategy\")[\"bias_reduction_percentage\"].mean().reset_index()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=strategy_bias[\"strategy\"], y=strategy_bias[\"bias_reduction_percentage\"],\n",
    "               name=\"Bias Reduction\", showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Success rates\n",
    "    success_rates = df.groupby(\"strategy\")[\"is_gender_neutral\"].mean().reset_index()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=success_rates[\"strategy\"], y=success_rates[\"is_gender_neutral\"],\n",
    "               name=\"Success Rate\", showlegend=False, marker_color=\"green\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Quality metrics\n",
    "    quality_metrics = df.groupby(\"strategy\")[[\"fluency_score\", \"bleu_4_score\"]].mean().reset_index()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=quality_metrics[\"strategy\"], y=quality_metrics[\"fluency_score\"],\n",
    "               name=\"Fluency\", marker_color=\"blue\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=quality_metrics[\"strategy\"], y=quality_metrics[\"bleu_4_score\"],\n",
    "               name=\"BLEU-4\", marker_color=\"orange\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df[\"bias_reduction_percentage\"], nbinsx=20,\n",
    "                    name=\"Distribution\", showlegend=False, marker_color=\"purple\"),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Performance Dashboard\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"üìà Performance dashboard generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Correlation Analysis\n",
    "if 'df' in locals():\n",
    "    # Calculate correlation matrix\n",
    "    metrics = [\"bias_reduction_percentage\", \"fluency_score\", \"bleu_4_score\", \n",
    "               \"semantic_similarity\", \"generation_time\"]\n",
    "    corr_matrix = df[metrics].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.3f')\n",
    "    plt.title('Correlation Matrix of Evaluation Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üîó Correlation analysis completed\")\n",
    "    \n",
    "    # Print key correlations\n",
    "    print(\"\\nüîç Key Correlations:\")\n",
    "    bias_fluency_corr = corr_matrix.loc['bias_reduction_percentage', 'fluency_score']\n",
    "    bias_bleu_corr = corr_matrix.loc['bias_reduction_percentage', 'bleu_4_score']\n",
    "    fluency_bleu_corr = corr_matrix.loc['fluency_score', 'bleu_4_score']\n",
    "    \n",
    "    print(f\"  Bias Reduction ‚Üî Fluency: {bias_fluency_corr:.3f}\")\n",
    "    print(f\"  Bias Reduction ‚Üî BLEU-4: {bias_bleu_corr:.3f}\")\n",
    "    print(f\"  Fluency ‚Üî BLEU-4: {fluency_bleu_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173b8ee",
   "metadata": {},
   "source": [
    "## 5. Key Findings and Interpretation\n",
    "\n",
    "Interpreting the statistical results and their implications for gender bias mitigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive findings summary\n",
    "def generate_findings_summary(df, anova_results):\n",
    "    \"\"\"Generate a comprehensive summary of findings\"\"\"\n",
    "    \n",
    "    findings = {\n",
    "        \"strategy_performance\": {},\n",
    "        \"statistical_significance\": {},\n",
    "        \"best_performers\": {},\n",
    "        \"trade_offs\": {},\n",
    "        \"success_rates\": {}\n",
    "    }\n",
    "    \n",
    "    # Strategy performance\n",
    "    for strategy in df['strategy'].unique():\n",
    "        strategy_data = df[df['strategy'] == strategy]\n",
    "        findings[\"strategy_performance\"][strategy] = {\n",
    "            \"bias_reduction\": {\n",
    "                \"mean\": strategy_data[\"bias_reduction_percentage\"].mean(),\n",
    "                \"std\": strategy_data[\"bias_reduction_percentage\"].std()\n",
    "            },\n",
    "            \"fluency\": {\n",
    "                \"mean\": strategy_data[\"fluency_score\"].mean(),\n",
    "                \"std\": strategy_data[\"fluency_score\"].std()\n",
    "            },\n",
    "            \"meaning_preservation\": {\n",
    "                \"mean\": strategy_data[\"bleu_4_score\"].mean(),\n",
    "                \"std\": strategy_data[\"bleu_4_score\"].std()\n",
    "            },\n",
    "            \"neutralization_success_rate\": strategy_data[\"is_gender_neutral\"].mean()\n",
    "        }\n",
    "    \n",
    "    # Statistical significance\n",
    "    for metric, result in anova_results.items():\n",
    "        findings[\"statistical_significance\"][metric] = {\n",
    "            \"significant\": result[\"significant\"],\n",
    "            \"p_value\": result[\"p_value\"],\n",
    "            \"effect_size\": result[\"eta_squared\"]\n",
    "        }\n",
    "    \n",
    "    # Best performers\n",
    "    strategy_means = df.groupby(\"strategy\").agg({\n",
    "        \"bias_reduction_percentage\": \"mean\",\n",
    "        \"fluency_score\": \"mean\", \n",
    "        \"bleu_4_score\": \"mean\",\n",
    "        \"is_gender_neutral\": \"mean\"\n",
    "    })\n",
    "    \n",
    "    findings[\"best_performers\"] = {\n",
    "        \"bias_reduction\": strategy_means[\"bias_reduction_percentage\"].idxmax(),\n",
    "        \"fluency\": strategy_means[\"fluency_score\"].idxmax(),\n",
    "        \"meaning_preservation\": strategy_means[\"bleu_4_score\"].idxmax(),\n",
    "        \"neutralization_success\": strategy_means[\"is_gender_neutral\"].idxmax()\n",
    "    }\n",
    "    \n",
    "    return findings\n",
    "\n",
    "if 'df' in locals() and 'anova_results' in locals():\n",
    "    findings = generate_findings_summary(df, anova_results)\n",
    "    \n",
    "    print(\"üîç KEY FINDINGS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Best performing strategies\n",
    "    print(f\"\\nüèÜ BEST PERFORMING STRATEGIES:\")\n",
    "    for metric, strategy in findings[\"best_performers\"].items():\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {strategy}\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    print(f\"\\nüìä STATISTICAL SIGNIFICANCE:\")\n",
    "    for metric, result in findings[\"statistical_significance\"].items():\n",
    "        significance = \"‚úÖ Significant\" if result[\"significant\"] else \"‚ùå Not significant\"\n",
    "        effect = \"Large\" if result[\"effect_size\"] > 0.14 else \"Medium\" if result[\"effect_size\"] > 0.06 else \"Small\"\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {significance} (p={result['p_value']:.4f}, Effect: {effect})\")\n",
    "    \n",
    "    # Strategy performance details\n",
    "    print(f\"\\nüìà DETAILED STRATEGY PERFORMANCE:\")\n",
    "    for strategy, performance in findings[\"strategy_performance\"].items():\n",
    "        print(f\"\\n  {strategy.upper()}:\")\n",
    "        print(f\"    Bias Reduction: {performance['bias_reduction']['mean']:.1f}% (¬±{performance['bias_reduction']['std']:.1f})\")\n",
    "        print(f\"    Fluency Score: {performance['fluency']['mean']:.3f} (¬±{performance['fluency']['std']:.3f})\")\n",
    "        print(f\"    BLEU-4 Score: {performance['meaning_preservation']['mean']:.3f} (¬±{performance['meaning_preservation']['std']:.3f})\")\n",
    "        print(f\"    Success Rate: {performance['neutralization_success_rate']:.1%}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Findings summary complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d8f11",
   "metadata": {},
   "source": [
    "## 6. Export and Reporting\n",
    "\n",
    "Exporting results for academic presentation and publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results for academic presentation\n",
    "def export_academic_results(df, anova_results, findings, output_dir):\n",
    "    \"\"\"Export results in formats suitable for academic presentation\"\"\"\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    exports = []\n",
    "    \n",
    "    # 1. Summary statistics table (CSV)\n",
    "    summary_stats = df.groupby(\"strategy\").agg({\n",
    "        \"bias_reduction_percentage\": [\"mean\", \"std\", \"count\"],\n",
    "        \"fluency_score\": [\"mean\", \"std\"],\n",
    "        \"bleu_4_score\": [\"mean\", \"std\"],\n",
    "        \"semantic_similarity\": [\"mean\", \"std\"],\n",
    "        \"is_gender_neutral\": [\"mean\"]\n",
    "    }).round(3)\n",
    "    \n",
    "    summary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns]\n",
    "    summary_file = output_path / \"summary_statistics.csv\"\n",
    "    summary_stats.to_csv(summary_file)\n",
    "    exports.append(summary_file)\n",
    "    \n",
    "    # 2. ANOVA results (JSON)\n",
    "    anova_file = output_path / \"anova_results.json\"\n",
    "    with open(anova_file, 'w') as f:\n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        anova_export = {}\n",
    "        for metric, result in anova_results.items():\n",
    "            anova_export[metric] = {\n",
    "                \"f_statistic\": float(result[\"f_statistic\"]),\n",
    "                \"p_value\": float(result[\"p_value\"]),\n",
    "                \"eta_squared\": float(result[\"eta_squared\"]),\n",
    "                \"significant\": bool(result[\"significant\"])\n",
    "            }\n",
    "        json.dump(anova_export, f, indent=2)\n",
    "    exports.append(anova_file)\n",
    "    \n",
    "    # 3. Complete dataset (CSV)\n",
    "    dataset_file = output_path / \"complete_dataset.csv\"\n",
    "    df.to_csv(dataset_file, index=False)\n",
    "    exports.append(dataset_file)\n",
    "    \n",
    "    # 4. Key findings report (Markdown)\n",
    "    findings_file = output_path / \"key_findings.md\"\n",
    "    with open(findings_file, 'w') as f:\n",
    "        f.write(\"# Gender Bias in LLMs - Key Findings\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(\"## Best Performing Strategies\\\\n\\\\n\")\n",
    "        for metric, strategy in findings[\"best_performers\"].items():\n",
    "            f.write(f\"- **{metric.replace('_', ' ').title()}**: {strategy}\\\\n\")\n",
    "        \n",
    "        f.write(\"\\\\n## Statistical Significance\\\\n\\\\n\")\n",
    "        for metric, result in findings[\"statistical_significance\"].items():\n",
    "            significance = \"Significant\" if result[\"significant\"] else \"Not significant\"\n",
    "            effect = \"Large\" if result[\"effect_size\"] > 0.14 else \"Medium\" if result[\"effect_size\"] > 0.06 else \"Small\"\n",
    "            f.write(f\"- **{metric.replace('_', ' ').title()}**: {significance} (p={result['p_value']:.4f}, Effect size: {effect})\\\\n\")\n",
    "        \n",
    "        f.write(\"\\\\n## Strategy Performance Summary\\\\n\\\\n\")\n",
    "        for strategy, performance in findings[\"strategy_performance\"].items():\n",
    "            f.write(f\"### {strategy.upper()}\\\\n\")\n",
    "            f.write(f\"- Bias Reduction: {performance['bias_reduction']['mean']:.1f}% (¬±{performance['bias_reduction']['std']:.1f})\\\\n\")\n",
    "            f.write(f\"- Fluency Score: {performance['fluency']['mean']:.3f} (¬±{performance['fluency']['std']:.3f})\\\\n\")\n",
    "            f.write(f\"- BLEU-4 Score: {performance['meaning_preservation']['mean']:.3f} (¬±{performance['meaning_preservation']['std']:.3f})\\\\n\")\n",
    "            f.write(f\"- Success Rate: {performance['neutralization_success_rate']:.1%}\\\\n\\\\n\")\n",
    "    \n",
    "    exports.append(findings_file)\n",
    "    \n",
    "    return exports\n",
    "\n",
    "# Export results\n",
    "if 'df' in locals() and 'anova_results' in locals() and 'findings' in locals():\n",
    "    export_dir = project_root / \"data\" / \"results\" / \"academic_export\"\n",
    "    \n",
    "    print(\"üì§ Exporting results for academic presentation...\")\n",
    "    exported_files = export_academic_results(df, anova_results, findings, export_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Exported {len(exported_files)} files:\")\n",
    "    for file_path in exported_files:\n",
    "        print(f\"  üìÑ {file_path.name}\")\n",
    "    \n",
    "    print(f\"\\\\nüìÅ All files saved to: {export_dir}\")\n",
    "    \n",
    "    # Create a citation-ready summary\n",
    "    print(f\"\\\\nüìù CITATION-READY SUMMARY:\")\n",
    "    print(f\"   Study: Gender Bias in Large Language Models\")\n",
    "    print(f\"   Total experiments: {len(df)}\")\n",
    "    print(f\"   Strategies tested: {df['strategy'].nunique()}\")\n",
    "    print(f\"   Models evaluated: {df['model'].nunique()}\")\n",
    "    print(f\"   Paragraphs analyzed: {df['paragraph_id'].nunique()}\")\n",
    "    \n",
    "    significant_metrics = [m for m, r in anova_results.items() if r['significant']]\n",
    "    print(f\"   Significant differences found in: {', '.join(significant_metrics) if significant_metrics else 'No metrics'}\")\n",
    "    \n",
    "    best_overall = df.groupby(\"strategy\")[\"bias_reduction_percentage\"].mean().idxmax()\n",
    "    best_score = df.groupby(\"strategy\")[\"bias_reduction_percentage\"].mean().max()\n",
    "    print(f\"   Best performing strategy: {best_overall} ({best_score:.1f}% bias reduction)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot export - missing analysis results. Please run the analysis cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10814466",
   "metadata": {},
   "source": [
    "## 7. Next Steps and Recommendations\n",
    "\n",
    "### For Academic Presentation:\n",
    "\n",
    "1. **Statistical Reporting**: Use the ANOVA results and effect sizes in your methodology section\n",
    "2. **Visualization**: Include the generated plots in your presentation/paper\n",
    "3. **Discussion Points**: Focus on the trade-offs between bias reduction and text quality\n",
    "4. **Limitations**: Discuss the scope of gendered terms detected and potential improvements\n",
    "\n",
    "### For Further Research:\n",
    "\n",
    "1. **Extended Corpus**: Test with more paragraphs and diverse domains\n",
    "2. **Additional Metrics**: Include human evaluation for bias detection\n",
    "3. **Cross-Model Analysis**: Compare performance across different LLM architectures\n",
    "4. **Temporal Analysis**: Study consistency across multiple runs\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- **Summary Statistics**: For methodology and results sections\n",
    "- **ANOVA Results**: For statistical significance reporting\n",
    "- **Complete Dataset**: For reproducibility and peer review\n",
    "- **Key Findings**: For discussion and conclusion sections\n",
    "\n",
    "---\n",
    "\n",
    "**Run all cells in sequence to reproduce the complete analysis!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
